--- a/src/agents/report_generator.py
+++ b/src/agents/report_generator.py
@@ -84,16 +84,44 @@ class ReportGeneratorAgent:
             # Check environment flag for generation mode
             use_llm_report = os.getenv('USE_LLM_REPORT', 'false').lower() == 'true'
             
-            if use_llm_report:
-                logger.info("Using LLM-powered report generation mode")
-                state = self.generate_with_llm(state)
-            else:
-                logger.info("Using template-based report generation mode")
-                state = self.generate_with_template(state)
+            if use_llm_report:
+                logger.info("Using LLM-powered report generation mode")
+                state = self.generate_with_llm(state)
+            else:
+                logger.info("Using template-based report generation mode")
+                state = self.generate_with_template(state)
             
             logger.info("Report generation completed successfully")
             
         except Exception as e:
             error_msg = f"Report generation failed: {str(e)}"
             logger.error(error_msg)
             state.add_error(error_msg)
         
         return state
     
-    def generate_with_llm(self, state: AnalysisState) -> AnalysisState:
-        """
-        LLM-powered dynamic report generation with fallback to template generation.
-        
-        Args:
-            state: Analysis state with all collected data
-            
-        Returns:
-            Updated state with final report
-        """
-        try:
-            # This would implement LLM-powered report generation
-            # For now, this is a stub that demonstrates the architecture
-            logger.info("LLM mode: Generating dynamic report with AI assistance")
-            
-            # Placeholder for LLM implementation:
-            # 1. Prepare structured data for LLM prompt
-            # 2. Create comprehensive prompt for report generation
-            # 3. Use LLM to generate personalized report sections
-            # 4. Parse and validate LLM output
-            # 5. Format as structured markdown/HTML
-            
-            # For demonstration, we'll simulate LLM failure and fallback
-            raise NotImplementedError("LLM report generation not yet available")
-            
-        except Exception as e:
-            logger.warning(f"LLM report generation failed: {str(e)}. Falling back to template generation.")
-            # Fallback to template-based generation
-            return self.generate_with_template(state)
+    def generate_with_llm(self, state: AnalysisState) -> AnalysisState:
+        """
+        LLM-powered dynamic report generation with fallback to template generation.
+        
+        Args:
+            state: Analysis state with all collected data
+            
+        Returns:
+            Updated state with final report
+        """
+        try:
+            logger.info("LLM mode: Generating dynamic report with AI assistance")
+            
+            # Try to use available LLM providers
+            report = None
+            
+            # Try Ollama first (local LLM)
+            logger.info("Checking if Ollama is available...")
+            if self._is_ollama_available():
+                logger.info("Ollama is available, attempting to generate report...")
+                report = self._generate_with_ollama(state)
+            else:
+                logger.info("Ollama is not available")
+            
+            # If Ollama not available, try Anthropic
+            if not report:
+                logger.info("Checking if Anthropic is available...")
+                if self._is_anthropic_available():
+                    logger.info("Anthropic is available, attempting to generate report...")
+                    report = self._generate_with_anthropic(state)
+                else:
+                    logger.info("Anthropic is not available")
+            
+            # If no LLM available, fall back to template
+            if not report:
+                logger.warning("No LLM available or LLM generation failed. Falling back to template generation.")
+                return self.generate_with_template(state)
+            
+            state.final_report = report
+            logger.info("LLM report generation completed successfully")
+            return state
+            
+        except Exception as e:
+            logger.warning(f"LLM report generation failed: {str(e)}. Falling back to template generation.")
+            logger.exception("Detailed error information:")
+            # Fallback to template-based generation
+            return self.generate_with_template(state)
     
     def generate_with_template(self, state: AnalysisState) -> AnalysisState: